{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMD04pRgH0eHxpR+2rWfEvP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iGrbjz1Imnw_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735283432675,"user_tz":-60,"elapsed":16332,"user":{"displayName":"Alper Sancılı","userId":"06880076798498529141"}},"outputId":"43a2381a-fedb-4882-8ba9-14cd5843dd43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average Estimated values: [0.19757649 0.49300118 0.69902319]\n","Average Counts: [ 56.027  81.501 862.472]\n","Average Cumulative Reward: 655.766\n","Average Regret: 44.23400000000004\n","Difference between true and average estimated values: [0.00242351 0.00699882 0.00097681]\n"]}],"source":["import numpy as np\n","\n","def epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs):\n","  Q = np.zeros(num_arms) #estimated rewards for each arm\n","  N = np.zeros(num_arms) #number of times each arm has been pulled\n","  rewards = [] # List to store rewards obtained at each step\n","\n","  def pull_arm(arm):\n","    return np.random.rand() < true_reward_probs[arm]\n","\n","  for t in range(num_plays):\n","    if np.random.rand() < epsilon: #exploration\n","      arm = np.random.choice(num_arms)\n","    else: #exploitation\n","      arm = np.argmax(Q)\n","\n","    reward = pull_arm(arm)\n","    rewards.append(reward)\n","\n","    N[arm] += 1\n","    Q[arm] += (reward - Q[arm]) / N[arm]\n","\n","  return Q, N, rewards\n","\n","#Parameters\n","num_arms = 3\n","num_plays = 1000\n","epsilon = 0.1\n","true_reward_probs = [0.2, 0.5, 0.7]\n","num_simulations = 1000\n","\n","#Run multiple simulations\n","all_Q = np.zeros((num_simulations, num_arms))\n","all_N = np.zeros((num_simulations, num_arms))\n","all_rewards = []\n","\n","for i in range(num_simulations):\n","  Q, N, rewards = epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs)\n","  all_Q[i] = Q\n","  all_N[i] = N\n","  all_rewards.append(sum(rewards))\n","\n","#Average results\n","avg_Q = np.mean(all_Q, axis=0)\n","avg_N = np.mean(all_N, axis=0)\n","avg_cumulative_reward = np.mean(all_rewards)\n","best_arm_reward = max(true_reward_probs) * num_plays\n","avg_regret = best_arm_reward - avg_cumulative_reward\n","\n","print(\"Average Estimated values:\", avg_Q)\n","print(\"Average Counts:\", avg_N)\n","print(\"Average Cumulative Reward:\", avg_cumulative_reward)\n","print(\"Average Regret:\", avg_regret)\n","print(\"Difference between true and average estimated values:\", np.abs(true_reward_probs - avg_Q))"]}]}